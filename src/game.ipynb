{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TicTacToe import TicTacToe\n",
    "from Players import RandomPlayer, RandomWinner, RandomWinnerBlocker, Player\n",
    "from functools import lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  X|\n",
      "|XOX|\n",
      "|OOO|\n",
      "\n",
      "\n",
      "Winner: O\n"
     ]
    }
   ],
   "source": [
    "p1 = RandomPlayer('O')\n",
    "p2 = RandomPlayer('X')\n",
    "\n",
    "game = TicTacToe()\n",
    "p1_next=True\n",
    "\n",
    "while True:\n",
    "    if p1_next:\n",
    "        p1.move(game)\n",
    "    else:\n",
    "        p2.move(game)\n",
    "        \n",
    "    p1_next = not p1_next\n",
    "    # game.print_board()\n",
    "    \n",
    "    if game.game_over():\n",
    "        game.print_board()\n",
    "\n",
    "        print('game over')\n",
    "        break\n",
    "    if game.winner():\n",
    "        game.print_board()\n",
    "\n",
    "        print(f\"Winner: {game.winner()}\")\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementTicTacToeLearner:\n",
    "\n",
    "    def __init__(self, n: int, epsilon: float, opponent: Player, player: str = 'X') -> None:\n",
    "\n",
    "        self.state_dict = dict()\n",
    "        self.epsilon = epsilon\n",
    "        self.n = n\n",
    "        self.player = player\n",
    "        self.oppoent = opponent\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache\n",
    "    def get_state_key(states):\n",
    "        return sorted(states)[0]\n",
    "\n",
    "    def decide_next_move(self, board: TicTacToe):\n",
    "\n",
    "        possible_moves = board.possible_moves()\n",
    "\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # do random move\n",
    "            return random.choice(possible_moves)\n",
    "\n",
    "        # else do greedy move\n",
    "        outcomes = [None]*len(possible_moves)\n",
    "\n",
    "        for idx, move in enumerate(possible_moves):\n",
    "            hyp_state = board.fake_move(player=self.player, index=move)\n",
    "            hyp_key = self.get_state_key(\n",
    "                game.similar_states(curr_state=hyp_state))\n",
    "\n",
    "            if hyp_key in self.state_dict:\n",
    "                outcomes[idx] = self.state_dict[hyp_key][0]\n",
    "            elif board.winner(curr_state=hyp_key) == self.player:\n",
    "                # next move would win\n",
    "                outcomes[idx] = 1\n",
    "            elif board.winner(curr_state=hyp_key):\n",
    "                # next move would lose\n",
    "                outcomes[idx] = 0\n",
    "            elif board.moves == 8:\n",
    "                # next move would end in draw\n",
    "                outcomes[idx] = 0\n",
    "            else:\n",
    "                # not seen before, default to 0.5\n",
    "                outcomes[idx] = 0.5\n",
    "\n",
    "        # if there are multiple optimal choices, we pick randomly from those\n",
    "        best_outcome = np.max(outcomes)\n",
    "        best_indexes = [idx for idx, outcome in enumerate(\n",
    "            outcomes) if outcome == best_outcome]\n",
    "        best_index = random.choice(best_indexes)\n",
    "\n",
    "        return possible_moves[best_index]\n",
    "\n",
    "    def play_one_game(self):\n",
    "\n",
    "        state_moves = []\n",
    "        game = TicTacToe()\n",
    "\n",
    "        if random.randint(0, 1):\n",
    "            # other player goes first\n",
    "            # print('2 going first')\n",
    "            game = self.oppoent.move(game)\n",
    "            played_first = False\n",
    "        else:\n",
    "            played_first = True\n",
    "\n",
    "        p1_turn = True\n",
    "\n",
    "        while not game.game_over() and not game.winner():\n",
    "\n",
    "            if p1_turn:\n",
    "                next_move = self.decide_next_move(board=game)\n",
    "                state_moves.append(game.fake_move(\n",
    "                    player=self.player, index=next_move))\n",
    "\n",
    "                game.add_move(player=self.player, index=next_move)\n",
    "            else:\n",
    "                game = self.oppoent.move(game)\n",
    "\n",
    "            p1_turn = not p1_turn\n",
    "\n",
    "        return game.winner(), state_moves, played_first\n",
    "\n",
    "    def get_result_and_increment(self, winner):\n",
    "        \"\"\"\n",
    "        returns: (result, increment)\n",
    "        \"\"\"\n",
    "        # gameover\n",
    "        if winner == self.player:\n",
    "            # we won\n",
    "            return 'w', 1\n",
    "        elif winner is False:\n",
    "            # game was a draw\n",
    "            return 'd', 0\n",
    "        else:\n",
    "            # we lost\n",
    "            return 'l', 0\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        wld = [None]*self.n\n",
    "        \"\"\"\n",
    "        state_dict looks like \n",
    "        {'state1': [probability, num_wins, num]}\n",
    "        \"\"\"\n",
    "        for game_num in range(0, self.n):\n",
    "\n",
    "            winner, state_moves, played_first = self.play_one_game()\n",
    "            result, increment = self.get_result_and_increment(winner)\n",
    "\n",
    "            wld[game_num] = (played_first, result)\n",
    "            \n",
    "            for state in state_moves:\n",
    "                str_state = ''.join(state)\n",
    "                state_key = self.get_state_key(game.similar_states(str_state))\n",
    "                if state_key in self.state_dict:\n",
    "                    prob, won, played = self.state_dict[state_key]\n",
    "                    new_won = won + increment\n",
    "                    new_played = played + 1\n",
    "\n",
    "                    self.state_dict[state_key] = [\n",
    "                        new_won/new_played, new_won, new_played]\n",
    "\n",
    "                else:\n",
    "                    self.state_dict[state_key] = [increment, increment, 1]\n",
    "\n",
    "        return self.state_dict, wld\n",
    "    \n",
    "    def play_n_games(self, n):\n",
    "        wld = [None]*n\n",
    "\n",
    "        for game_num in range(n):\n",
    "            winner, _, played_first = self.play_one_game()\n",
    "            result, _ = self.get_result_and_increment(winner)\n",
    "            wld[game_num] = (played_first, result)\n",
    "\n",
    "        return wld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game.print_board()\n",
    "# for k,v in state_dict.items():\n",
    "#     print(k,v)\n",
    "\n",
    "ws = np.cumsum([1 if x=='w' else 0 for x in wld ])\n",
    "ds = np.cumsum([1 if x=='l' else 0 for x in wld ])\n",
    "ls = np.cumsum([1 if x=='d' else 0 for x in wld ])\n",
    "\n",
    "\n",
    "print(f\"W: {ws[-1]}, D: {ds[-1]}, L: {ls[-1]}\")\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(ws,label='w')\n",
    "ax.plot(ds,label='d')\n",
    "ax.plot(ls,label='l')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlearner = ReinforcementTicTacToeLearner(100000, 0.05, RandomPlayer(player='O'), player='X')\n",
    "state, wld = rlearner.learn()\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9389"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_games=10000\n",
    "wld = rlearner.play_n_games(num_games)\n",
    "x = len(list(filter(lambda x: x[1]=='w',wld)))/num_games\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wld(wld):\n",
    "\n",
    "    def subplot(wld_arr, ax):\n",
    "            \n",
    "        ws = np.cumsum([1 if x=='w' else 0 for x in wld_arr ])\n",
    "        ds = np.cumsum([1 if x=='l' else 0 for x in wld_arr ])\n",
    "        ls = np.cumsum([1 if x=='d' else 0 for x in wld_arr ])\n",
    "\n",
    "        ax.plot(ws,label='w')\n",
    "        ax.plot(ds,label='d')\n",
    "        ax.plot(ls,label='l')\n",
    "        \n",
    "        return ax\n",
    "    \n",
    "    first = list(filter(lambda x: x[0] is True, wld))\n",
    "    second = list(filter(lambda x: x[0] is False, wld))\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2)\n",
    "\n",
    "    axs[0] = subplot(first, axs[0])\n",
    "    axs[1] = subplot(second, axs[1])\n",
    "\n",
    "    axs[0].set_title('Played First')\n",
    "    axs[1].set_title('Played Second')\n",
    "\n",
    "    return fig, axs    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wld(wld)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11def4143cd92162dbb305650f4e8ef789c713539a86da1e1622b80cccc83034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
